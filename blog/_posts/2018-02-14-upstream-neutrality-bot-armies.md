---
title: The Bots Have Arrived
layout: post
author: Nate Levesque
tags: [upstream-neutrality]
head-image: robot-arm.jpg
---

It's hard to know exactly who, or what, an online persona actually is. We're relatively sure that the people we know in real life are who they claim to be online (though their online life is likely nicer than their real one), but as for anyone else, it's anyone's guess. The fact that some larger social media sites, Facebook, Twitter, and Instagram in particular, are also home to the persona of brands further blurs the line of what's real and what's not. Then, there are the bots.

Social media bots come in a variety of flavors. At their simplest, they're helpful and provide things like unit conversions, periodic market prices, or other non-conversational information. At their more complex, they stand in for brands or people, tweeting about current news and even trying to talk to other users. With the more complex bots, it can be hard, if not impossible, to tell a bot from a real person. Due to that difficulty and the ability to manipulate online conversation by influencing trending topics (which is possible to do with enough bots), we're entering a danger zone of manufactured conversation and artificially influenced views online.

It's possible to buy access to bots and there is suspicion that some brands do that to inflate their social media influence. Not only that, but there's an entire online industry devoted to fake or bot accounts that follow, "like", and comment on content to boost its social media presence. A Forbes contributor [describes the effect as "Bot Rot."](https://www.forbes.com/sites/michaelstone/2018/02/09/the-bot-rot-are-follower-numbers-real/#4335e5ed6485) We don't reliably know how many social media users are real, though some platforms are more effected than others. In summer 2017, a security researcher published a study indicating that [millions of Instagram users are actually bots](https://motherboard.vice.com/en_us/article/wnj9vy/24-million-instagram-accounts-spambots-study). Twitter has [shut down millions of bot accounts](https://mashable.com/2018/01/29/twitter-bots-purge-influencers-accounts) over the past year and it's suspected that [at least half of Trump's followers are bots](http://www.newsweek.com/donald-trump-twitter-followers-fake-617873).

The bot problem has implications for propaganda as bots have gotten more complex and bigger actors have learned to use them. In 2011, the [United States Central Command awarded a contract](https://www.wired.co.uk/article/robot-propaganda) for an "online persona management service" to a firm in California which included fake online profiles—effectively a bot army—so we know that even state actors have taken an interest.

While it may seem a bit far-fetched that bot armies could be manipulating online conversation, it appears that it's already happening and in alarming ways. In 2015, [over 75,000 online bots](https://motherboard.vice.com/en_us/article/z4maww/how-mexican-twitter-bots-shut-down-dissent) were used to fight protests and critics of the Mexican government. Those bots appeared online in 2012 and were used to spam hashtags being used to document human rights abuses, among other things. Recently, an army of bots that spread fake news on a wide scale was discovered to have been [operating during the 2016 election](https://www.npr.org/sections/alltechconsidered/2017/04/03/522503844/how-russian-twitter-bots-pumped-out-fake-news-during-the-2016-election) (and there are now online tools to see if you interacted with any). Just this year, in 2018, [it was reported](https://www.thepeninsulaqatar.com/article/05/02/2018/Saudi-bots-use-%E2%80%98hashtag-poisoning%E2%80%99-to-spread-propaganda) that bot armies were being used to beat down dissent against the Saudi Arabian government.

Although it's hard to believe that we could be influenced by armies of bots, we may be more impressionable than we think - and it's really inexpensive to buy a bot army. The Daily Beast, a news outlet, [bought access](https://www.thedailybeast.com/i-bought-a-russian-bot-army-for-under-dollar100) to a Russian bot army of 1,000 accounts for just $45 and found they could buy software to control it for $250. Bot armies are advertised for anywhere from the $45 The Daily Beast spent, to more for accounts that have existed longer or otherwise seem to be more legitimate. It takes far less than that to influence the conversation. [MIT found](https://www.wired.co.uk/article/robot-propaganda) that a single upvote on a story improves the response to it by 25% and an early downvote can make it be seen as a bad article. Facebook even [experimented with manipulating](https://www.independent.co.uk/life-style/gadgets-and-tech/facebook-manipulated-users-moods-in-secret-experiment-9571004.html) its user's moods by changing what words were seen in their feeds.

Bots are not all bad. Armies of bots patrol Wikipedia, Reddit, and other sites, blocking malicious edits, moderating hate speech, and answering questions. But, armies of bots are also working to influence what we talk about and what we see online by poisoning hashtags, discussing fake news amongst themselves, and by voting and commenting on content. For the untrained eye, it can be very hard to tell what's real.